!pip uninstall -y whisper
!pip install -q openai-whisper
!pip install -U google-generativeai

from google.colab import files

print("ðŸ“¤ Upload your podcast audio file (MP3/WAV)")
uploaded = files.upload()

audio_path = list(uploaded.keys())[0]
print("âœ… Uploaded file:", audio_path)


import sys

# Force clean import of whisper
if 'whisper' in sys.modules:
    del sys.modules['whisper']

import whisper

whisper_model = whisper.load_model("base")
print("âœ… Whisper model loaded successfully")

result = whisper_model.transcribe(
    audio_path,
    word_timestamps=True,
    verbose=False
)

print("âœ… Transcription completed")
print(f"Total segments: {len(result['segments'])}")

# -------- Create time-based chunks (max 2 minutes each) --------

chunks = []
MAX_DURATION = 120  # seconds

segments = result["segments"]

current_chunk = {
    "start": segments[0]["start"],
    "end": segments[0]["end"],
    "text": segments[0]["text"]
}

for segment in segments[1:]:
    # If adding this segment keeps chunk under max duration
    if segment["end"] - current_chunk["start"] <= MAX_DURATION:
        current_chunk["text"] += " " + segment["text"]
        current_chunk["end"] = segment["end"]
    else:
        # Save current chunk and start new one
        chunks.append(current_chunk)
        current_chunk = {
            "start": segment["start"],
            "end": segment["end"],
            "text": segment["text"]
        }

# Add last chunk
chunks.append(current_chunk)

print(f"âœ… Chunks created: {len(chunks)}")

# Quick check
print("First chunk preview:")
print(f"[{chunks[0]['start']:.2f}s â€“ {chunks[0]['end']:.2f}s]")
print(chunks[0]['text'][:150], "...")

import google.generativeai as genai

# ðŸ”‘ Configure Gemini (PASTE YOUR API KEY)
genai.configure(api_key="PASTE YOUR API KEY")

llm_model = genai.GenerativeModel("models/gemini-2.5-flash")
print("âœ… Gemini model initialized")


# Prepare chunk text for LLM
chunks_text = ""

for i, c in enumerate(chunks, 1):
    chunks_text += f"""
Segment {i}
Start Time: {int(c['start'])}s
End Time: {int(c['end'])}s
Text:
{c['text']}
"""

prompt = f"""
You are an NLP expert.

For each segment below, generate:
- Topic Title
- Start Time (hh:mm:ss)
- End Time (hh:mm:ss)
- 4â€“6 Keywords
- Short Summary (2â€“3 lines)

Use EXACTLY this format:

Segment X
Title:
Start Time:
End Time:
Keywords:
Summary:

Segments:
{chunks_text}
"""

response = llm_model.generate_content(prompt)
structured_output = response.text

print(structured_output)


with open("topic_wise_transcript.txt", "w", encoding="utf-8") as f:
    f.write(structured_output)

print("âœ… File saved successfully")


from google.colab import files
files.download("topic_wise_transcript.txt")


import json

topic_json = [
    {
        "segment": "Segment 1",
        "title": "Introduction to University History & Bologna's Founding",
        "start_time": "00:00:04",
        "end_time": "00:02:04",
        "keywords": [
            "Mini-series",
            "university history",
            "higher education",
            "medieval period",
            "Bologna University",
            "oldest institution"
        ],
        "summary": "The host introduces a three-part mini-series on the history of universities, highlighting Bologna as one of the oldest surviving institutions."
    },
    {
        "segment": "Segment 2",
        "title": "The Informal Beginnings and Drivers of Early Universities",
        "start_time": "00:02:05",
        "end_time": "00:04:00",
        "keywords": [
            "Early universities",
            "informal gatherings",
            "medieval Europe",
            "city-states",
            "legal expertise",
            "Islamic world"
        ],
        "summary": "This segment explains how early universities emerged informally due to social stability, trade growth, and the demand for legal education."
    }
    # âž• Continue till Segment 12
]


with open("topic_wise_transcript.json", "w", encoding="utf-8") as f:
    json.dump(topic_json, f, indent=2, ensure_ascii=False)

print("âœ… topic_wise_transcript.json created")


files.download("topic_wise_transcript.json")


import re
import json

def parse_structured_output(text):
    segments = []
    blocks = re.split(r"\n(?=Segment \d+)", text.strip())

    for block in blocks:
        segment = {}

        seg_match = re.search(r"(Segment \d+)", block)
        title = re.search(r"Title:\s*(.*)", block)
        start = re.search(r"Start Time:\s*(.*)", block)
        end = re.search(r"End Time:\s*(.*)", block)
        keywords = re.search(r"Keywords:\s*(.*)", block)
        summary = re.search(r"Summary:\s*(.*)", block, re.S)

        if seg_match:
            segment["segment"] = seg_match.group(1)
        if title:
            segment["title"] = title.group(1).strip()
        if start:
            segment["start_time"] = start.group(1).strip()
        if end:
            segment["end_time"] = end.group(1).strip()
        if keywords:
            segment["keywords"] = [k.strip() for k in keywords.group(1).split(",")]
        if summary:
            segment["summary"] = summary.group(1).strip()

        segments.append(segment)

    return segments

# Convert Gemini output â†’ JSON
topic_json = parse_structured_output(structured_output)

# Save JSON
with open("topic_wise_transcript.json", "w", encoding="utf-8") as f:
    json.dump(topic_json, f, indent=2, ensure_ascii=False)

print("âœ… FULL topic_wise_transcript.json created")


from google.colab import files
files.download("topic_wise_transcript.json")
